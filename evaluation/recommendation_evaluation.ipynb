{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study Program Recommender System - Evaluation Notebook\n",
    "\n",
    "This notebook provides tools to evaluate the recommendation system using standard IR metrics:\n",
    "- **NDCG@k**: Normalized Discounted Cumulative Gain\n",
    "- **Precision@k**: Proportion of relevant items in top-k recommendations\n",
    "- **CTR**: Click-Through Rate\n",
    "- **Acceptance Rate**: Percentage of recommendations accepted\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from supabase import create_client\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supabase = create_client(\n",
    "    os.getenv('SUPABASE_URL'),\n",
    "    os.getenv('SUPABASE_ANON_KEY')\n",
    ")\n",
    "\n",
    "print(\"Connected to Supabase successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(relevance_scores: List[float], k: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Discounted Cumulative Gain at k.\n",
    "    \n",
    "    Args:\n",
    "        relevance_scores: List of relevance scores (higher is better)\n",
    "        k: Number of top results to consider\n",
    "    \n",
    "    Returns:\n",
    "        DCG@k score\n",
    "    \"\"\"\n",
    "    relevance_scores = np.array(relevance_scores)[:k]\n",
    "    if len(relevance_scores) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    discounts = np.log2(np.arange(2, len(relevance_scores) + 2))\n",
    "    return np.sum(relevance_scores / discounts)\n",
    "\n",
    "\n",
    "def ndcg_at_k(predicted_relevance: List[float], ideal_relevance: List[float], k: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Normalized Discounted Cumulative Gain at k.\n",
    "    \n",
    "    Args:\n",
    "        predicted_relevance: Relevance scores in predicted order\n",
    "        ideal_relevance: Relevance scores in ideal (sorted) order\n",
    "        k: Number of top results to consider\n",
    "    \n",
    "    Returns:\n",
    "        NDCG@k score (0 to 1, higher is better)\n",
    "    \"\"\"\n",
    "    dcg = dcg_at_k(predicted_relevance, k)\n",
    "    idcg = dcg_at_k(sorted(ideal_relevance, reverse=True), k)\n",
    "    \n",
    "    if idcg == 0.0:\n",
    "        return 0.0\n",
    "    \n",
    "    return dcg / idcg\n",
    "\n",
    "\n",
    "def precision_at_k(predicted_items: List[str], relevant_items: List[str], k: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Precision at k.\n",
    "    \n",
    "    Args:\n",
    "        predicted_items: List of predicted item IDs\n",
    "        relevant_items: List of relevant item IDs\n",
    "        k: Number of top results to consider\n",
    "    \n",
    "    Returns:\n",
    "        Precision@k score (0 to 1, higher is better)\n",
    "    \"\"\"\n",
    "    if k == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    predicted_at_k = set(predicted_items[:k])\n",
    "    relevant_set = set(relevant_items)\n",
    "    \n",
    "    num_relevant_in_k = len(predicted_at_k.intersection(relevant_set))\n",
    "    \n",
    "    return num_relevant_in_k / k\n",
    "\n",
    "\n",
    "def recall_at_k(predicted_items: List[str], relevant_items: List[str], k: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Recall at k.\n",
    "    \n",
    "    Args:\n",
    "        predicted_items: List of predicted item IDs\n",
    "        relevant_items: List of relevant item IDs\n",
    "        k: Number of top results to consider\n",
    "    \n",
    "    Returns:\n",
    "        Recall@k score (0 to 1, higher is better)\n",
    "    \"\"\"\n",
    "    if len(relevant_items) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    predicted_at_k = set(predicted_items[:k])\n",
    "    relevant_set = set(relevant_items)\n",
    "    \n",
    "    num_relevant_in_k = len(predicted_at_k.intersection(relevant_set))\n",
    "    \n",
    "    return num_relevant_in_k / len(relevant_items)\n",
    "\n",
    "\n",
    "def mean_reciprocal_rank(predicted_items_list: List[List[str]], relevant_items_list: List[List[str]]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Mean Reciprocal Rank.\n",
    "    \n",
    "    Args:\n",
    "        predicted_items_list: List of predicted item lists for each query\n",
    "        relevant_items_list: List of relevant item lists for each query\n",
    "    \n",
    "    Returns:\n",
    "        MRR score (0 to 1, higher is better)\n",
    "    \"\"\"\n",
    "    reciprocal_ranks = []\n",
    "    \n",
    "    for predicted, relevant in zip(predicted_items_list, relevant_items_list):\n",
    "        relevant_set = set(relevant)\n",
    "        \n",
    "        for rank, item in enumerate(predicted, start=1):\n",
    "            if item in relevant_set:\n",
    "                reciprocal_ranks.append(1.0 / rank)\n",
    "                break\n",
    "        else:\n",
    "            reciprocal_ranks.append(0.0)\n",
    "    \n",
    "    return np.mean(reciprocal_ranks) if reciprocal_ranks else 0.0\n",
    "\n",
    "\n",
    "print(\"Evaluation metrics functions loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Feedback Data from Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feedback_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load feedback data from Supabase.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with feedback data\n",
    "    \"\"\"\n",
    "    result = supabase.table('feedback').select('*').execute()\n",
    "    return pd.DataFrame(result.data)\n",
    "\n",
    "\n",
    "def load_recommendations_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load recommendations data from Supabase.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with recommendations data\n",
    "    \"\"\"\n",
    "    result = supabase.table('recommendations').select('*').execute()\n",
    "    return pd.DataFrame(result.data)\n",
    "\n",
    "\n",
    "feedback_df = load_feedback_data()\n",
    "recommendations_df = load_recommendations_data()\n",
    "\n",
    "print(f\"Loaded {len(feedback_df)} feedback records\")\n",
    "print(f\"Loaded {len(recommendations_df)} recommendation records\")\n",
    "\n",
    "if len(feedback_df) > 0:\n",
    "    print(\"\\nFeedback data sample:\")\n",
    "    display(feedback_df.head())\n",
    "else:\n",
    "    print(\"\\nNo feedback data available yet. Generate some recommendations and collect feedback first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate User Engagement Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_engagement_metrics(feedback_df: pd.DataFrame, recommendations_df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate user engagement metrics from feedback and recommendations data.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with engagement metrics\n",
    "    \"\"\"\n",
    "    if len(recommendations_df) == 0:\n",
    "        return {\n",
    "            'total_recommendations': 0,\n",
    "            'total_clicks': 0,\n",
    "            'total_accepts': 0,\n",
    "            'ctr': 0.0,\n",
    "            'acceptance_rate': 0.0,\n",
    "            'avg_rating': 0.0\n",
    "        }\n",
    "    \n",
    "    total_recommendations = len(recommendations_df)\n",
    "    \n",
    "    total_clicks = feedback_df['clicked'].sum() if 'clicked' in feedback_df.columns and len(feedback_df) > 0 else 0\n",
    "    total_accepts = feedback_df['accepted'].sum() if 'accepted' in feedback_df.columns and len(feedback_df) > 0 else 0\n",
    "    \n",
    "    ctr = (total_clicks / total_recommendations) * 100 if total_recommendations > 0 else 0.0\n",
    "    acceptance_rate = (total_accepts / total_recommendations) * 100 if total_recommendations > 0 else 0.0\n",
    "    \n",
    "    ratings = feedback_df[feedback_df['rating'].notna()]['rating'] if 'rating' in feedback_df.columns and len(feedback_df) > 0 else []\n",
    "    avg_rating = ratings.mean() if len(ratings) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'total_recommendations': int(total_recommendations),\n",
    "        'total_clicks': int(total_clicks),\n",
    "        'total_accepts': int(total_accepts),\n",
    "        'ctr': float(ctr),\n",
    "        'acceptance_rate': float(acceptance_rate),\n",
    "        'avg_rating': float(avg_rating),\n",
    "        'num_ratings': int(len(ratings))\n",
    "    }\n",
    "\n",
    "\n",
    "metrics = calculate_engagement_metrics(feedback_df, recommendations_df)\n",
    "\n",
    "print(\"\\n=== User Engagement Metrics ===\")\n",
    "print(f\"Total Recommendations: {metrics['total_recommendations']}\")\n",
    "print(f\"Total Clicks: {metrics['total_clicks']}\")\n",
    "print(f\"Total Accepts: {metrics['total_accepts']}\")\n",
    "print(f\"Click-Through Rate: {metrics['ctr']:.2f}%\")\n",
    "print(f\"Acceptance Rate: {metrics['acceptance_rate']:.2f}%\")\n",
    "print(f\"Average Rating: {metrics['avg_rating']:.2f}/5.0 ({metrics['num_ratings']} ratings)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Recommendation Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recommendations(recommendations_df: pd.DataFrame, feedback_df: pd.DataFrame, k_values: List[int] = [1, 3, 5, 10]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate recommendation quality using NDCG@k and Precision@k.\n",
    "    \n",
    "    Uses feedback to determine relevance:\n",
    "    - Accepted: 3 points\n",
    "    - Clicked: 1 point\n",
    "    - Rated: rating/5 * 2 points\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with metrics for each k value\n",
    "    \"\"\"\n",
    "    if len(recommendations_df) == 0 or len(feedback_df) == 0:\n",
    "        print(\"Not enough data for evaluation. Need both recommendations and feedback.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    feedback_map = {}\n",
    "    for _, row in feedback_df.iterrows():\n",
    "        key = (row['student_id'], row['program_id'])\n",
    "        score = 0.0\n",
    "        \n",
    "        if row.get('clicked', False):\n",
    "            score += 1.0\n",
    "        if row.get('accepted', False):\n",
    "            score += 3.0\n",
    "        if pd.notna(row.get('rating')):\n",
    "            score += (row['rating'] / 5.0) * 2.0\n",
    "        \n",
    "        feedback_map[key] = score\n",
    "    \n",
    "    student_recs = recommendations_df.groupby('student_id')\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for k in k_values:\n",
    "        ndcg_scores = []\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        \n",
    "        for student_id, group in student_recs:\n",
    "            sorted_group = group.sort_values('score', ascending=False)\n",
    "            \n",
    "            predicted_programs = sorted_group['program_id'].tolist()\n",
    "            \n",
    "            relevance_scores = [\n",
    "                feedback_map.get((student_id, prog_id), 0.0)\n",
    "                for prog_id in predicted_programs\n",
    "            ]\n",
    "            \n",
    "            relevant_programs = [\n",
    "                prog_id for prog_id in predicted_programs\n",
    "                if feedback_map.get((student_id, prog_id), 0.0) > 0\n",
    "            ]\n",
    "            \n",
    "            if len(relevant_programs) > 0:\n",
    "                ndcg = ndcg_at_k(relevance_scores, relevance_scores, k)\n",
    "                ndcg_scores.append(ndcg)\n",
    "                \n",
    "                precision = precision_at_k(predicted_programs, relevant_programs, k)\n",
    "                precision_scores.append(precision)\n",
    "                \n",
    "                recall = recall_at_k(predicted_programs, relevant_programs, k)\n",
    "                recall_scores.append(recall)\n",
    "        \n",
    "        results.append({\n",
    "            'k': k,\n",
    "            'ndcg': np.mean(ndcg_scores) if ndcg_scores else 0.0,\n",
    "            'precision': np.mean(precision_scores) if precision_scores else 0.0,\n",
    "            'recall': np.mean(recall_scores) if recall_scores else 0.0,\n",
    "            'num_users': len(ndcg_scores)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "evaluation_results = evaluate_recommendations(recommendations_df, feedback_df)\n",
    "\n",
    "if len(evaluation_results) > 0:\n",
    "    print(\"\\n=== Recommendation Quality Metrics ===\")\n",
    "    display(evaluation_results)\n",
    "else:\n",
    "    print(\"\\nNot enough data for evaluation. Collect more feedback first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(evaluation_results) > 0:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    axes[0].plot(evaluation_results['k'], evaluation_results['ndcg'], marker='o', linewidth=2, markersize=8)\n",
    "    axes[0].set_xlabel('k', fontsize=12)\n",
    "    axes[0].set_ylabel('NDCG@k', fontsize=12)\n",
    "    axes[0].set_title('NDCG@k - Ranking Quality', fontsize=14, fontweight='bold')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].plot(evaluation_results['k'], evaluation_results['precision'], marker='s', linewidth=2, markersize=8, color='green')\n",
    "    axes[1].set_xlabel('k', fontsize=12)\n",
    "    axes[1].set_ylabel('Precision@k', fontsize=12)\n",
    "    axes[1].set_title('Precision@k - Relevance Rate', fontsize=14, fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[2].plot(evaluation_results['k'], evaluation_results['recall'], marker='^', linewidth=2, markersize=8, color='orange')\n",
    "    axes[2].set_xlabel('k', fontsize=12)\n",
    "    axes[2].set_ylabel('Recall@k', fontsize=12)\n",
    "    axes[2].set_title('Recall@k - Coverage', fontsize=14, fontweight='bold')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data to visualize. Collect feedback first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engagement Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(feedback_df) > 0 and 'created_at' in feedback_df.columns:\n",
    "    feedback_df['created_at'] = pd.to_datetime(feedback_df['created_at'])\n",
    "    feedback_df['date'] = feedback_df['created_at'].dt.date\n",
    "    \n",
    "    daily_engagement = feedback_df.groupby('date').agg({\n",
    "        'clicked': 'sum',\n",
    "        'accepted': 'sum',\n",
    "        'program_id': 'count'\n",
    "    }).rename(columns={'program_id': 'total'})\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    daily_engagement.plot(kind='bar', ax=ax)\n",
    "    ax.set_xlabel('Date', fontsize=12)\n",
    "    ax.set_ylabel('Count', fontsize=12)\n",
    "    ax.set_title('User Engagement Over Time', fontsize=14, fontweight='bold')\n",
    "    ax.legend(['Clicks', 'Accepts', 'Total Interactions'])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No temporal data available for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Evaluation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_evaluation_report(metrics: Dict, evaluation_results: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Generate a text report of evaluation results.\n",
    "    \"\"\"\n",
    "    report = []\n",
    "    report.append(\"=\" * 60)\n",
    "    report.append(\"STUDY PROGRAM RECOMMENDER - EVALUATION REPORT\")\n",
    "    report.append(\"=\" * 60)\n",
    "    report.append(\"\")\n",
    "    \n",
    "    report.append(\"USER ENGAGEMENT METRICS\")\n",
    "    report.append(\"-\" * 60)\n",
    "    report.append(f\"Total Recommendations: {metrics['total_recommendations']}\")\n",
    "    report.append(f\"Click-Through Rate: {metrics['ctr']:.2f}%\")\n",
    "    report.append(f\"Acceptance Rate: {metrics['acceptance_rate']:.2f}%\")\n",
    "    report.append(f\"Average Rating: {metrics['avg_rating']:.2f}/5.0\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    if len(evaluation_results) > 0:\n",
    "        report.append(\"RECOMMENDATION QUALITY METRICS\")\n",
    "        report.append(\"-\" * 60)\n",
    "        for _, row in evaluation_results.iterrows():\n",
    "            report.append(f\"k={row['k']:2d}: NDCG={row['ndcg']:.4f}, Precision={row['precision']:.4f}, Recall={row['recall']:.4f}\")\n",
    "        report.append(\"\")\n",
    "    \n",
    "    report.append(\"=\" * 60)\n",
    "    \n",
    "    return \"\\n\".join(report)\n",
    "\n",
    "\n",
    "report = generate_evaluation_report(metrics, evaluation_results)\n",
    "print(report)\n",
    "\n",
    "with open('evaluation_report.txt', 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\"\\nReport saved to evaluation_report.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides comprehensive evaluation tools for the recommendation system:\n",
    "\n",
    "1. **NDCG@k**: Measures ranking quality, considering position and relevance\n",
    "2. **Precision@k**: Measures what proportion of recommendations are relevant\n",
    "3. **Recall@k**: Measures what proportion of relevant items are recommended\n",
    "4. **CTR & Acceptance**: Measures user engagement with recommendations\n",
    "\n",
    "Use this notebook regularly to monitor system performance and identify areas for improvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
